---
---
Publications
==========

@inproceedings{higy_few-shot_2019,
	title = {Few-shot learning with attention-based sequence-to-sequence models},
	url = {http://arxiv.org/abs/1811.03519},
	abstract = {End-to-end approaches have recently become popular as a means of simplifying the training and deployment of speech recognition systems. However, they often require large amounts of data to perform well on large vocabulary tasks. With the aim of making end-to-end approaches usable by a broader range of researchers, we explore the potential to use end-to-end methods in small vocabulary contexts where smaller datasets may be used. A significant drawback of small-vocabulary systems is the difficulty of expanding the vocabulary beyond the original training samples -- therefore we also study strategies to extend the vocabulary with only few examples per new class (few-shot learning). Our results show that an attention-based encoder-decoder can be competitive against a strong baseline on a small vocabulary keyword classification task, reaching 97.5\% of accuracy on Tensorflow's Speech Commands dataset. It also shows promising results on the few-shot learning problem where a simple strategy achieved 34.8\% of accuracy on new keywords with only 10 examples for each new class. This score goes up to 80.3\% with a larger set of 100 examples.},
	urldate = {2018-11-09},
	author = {Higy, Bertrand and Bell, Peter},
	year = {2019},
	note = {arXiv: 1811.03519},
	keywords = {Computer Science - Computation and Language},
	file = {Higy and Bell - 2019 - Few-shot learning with attention-based sequence-to.pdf:/home/bjrhigy/Zotero/storage/J85IWFIF/Higy and Bell - 2019 - Few-shot learning with attention-based sequence-to.pdf:application/pdf},
}

@techreport{higy_representations_2010,
	title = {Repr{\'e}sentations collectives, langage et syst{\`e}mes de tagging},
	language = {fr},
	author = {Higy, Bertrand},
	month = jun,
	year = {2010},
	pages = {42},
	file = {Higy - 2010 - Repr{\'e}sentations collectives, langage et syst{\`e}mes d.pdf:/home/bjrhigy/Zotero/storage/USWY6LYY/Higy - 2010 - Repr{\'e}sentations collectives, langage et syst{\`e}mes d.pdf:application/pdf},
}

@techreport{higy_modelisation_2009,
	type = {Master thesis},
	title = {Mod{\'e}lisation connexioniste de l'int{\'e}gration multimodale},
	author = {Higy, Bertrand},
	month = jun,
	year = {2009},
	file = {Higy - 2009 - Mod{\'e}lisation connexioniste de l'int{\'e}gration multim.pdf:/home/bjrhigy/Zotero/storage/TF9BZVF3/Higy - 2009 - Mod{\'e}lisation connexioniste de l'int{\'e}gration multim.pdf:application/pdf},
}

@article{higy_speech_2018,
	title = {Speech {Recognition} for the {iCub} {Platform}},
	volume = {5},
	issn = {2296-9144},
	url = {https://www.frontiersin.org/articles/10.3389/frobt.2018.00010/full},
	doi = {10.3389/frobt.2018.00010},
	abstract = {This paper describes open source software (available at https://github.com/robotology/natural- speech) to build automatic speech recognition (ASR) systems and run them within the YARP platform. The toolkit is designed (i) to allow non-ASR experts to easily create their own ASR system and run it on iCub, and (ii) to build deep learning-based models specifically addressing the main challenges an ASR system faces in the context of verbal human-iCub interactions. The toolkit mostly consists of Python, C++ code and shell scripts integrated in YARP. As additional contribution, a second codebase (written in Matlab) is provided for more expert ASR users who want to experiment with bio-inspired and developmental learning-inspired ASR systems. Specifically, we provide code for two distinct kinds of speech recognition: {\textquotedblleft}articulatory{\textquotedblright} and {\textquotedblleft}unsupervised{\textquotedblright} speech recognition. The first is largely inspired by influential neurobiological theories of speech perception which assume speech perception to be mediated by brain motor cortex activities. Our articulatory systems have been shown to outperform strong deep learning- based baselines. The second type of recognition systems, the {\textquotedblleft}unsupervised{\textquotedblright} systems, do not use any supervised information (contrary to most ASR systems, including our articulatory systems). To some extent, they mimic an infant who has to discover the basic speech units of a language by herself. In addition, we provide resources consisting of pre-trained deep learning models for ASR, and a 2,5-hours speech dataset of spoken commands, the VoCub dataset, which can be used to adapt an ASR system to the typical acoustic environments in which iCub operates.},
	urldate = {2018-03-26},
	journal = {Frontiers in Robotics and AI},
	author = {Higy, Bertrand and Mereta, Alessio and Metta, Giorgio and Badino, Leonardo},
	year = {2018},
	keywords = {automatic speech recognition, C++, MATLAB, python, Tensorflow, YARP},
	file = {frontiers_2017.zip:/home/bjrhigy/Zotero/storage/UQT9RBVE/frontiers_2017.zip:application/zip;Higy et al. - 2018 - Speech Recognition for the iCub Platform.pdf:/home/bjrhigy/Zotero/storage/AGU9DANY/Higy et al. - 2018 - Speech Recognition for the iCub Platform.pdf:application/pdf},
}

@inproceedings{higy_combining_2016,
	address = {Cancun, Mexico},
	title = {Combining sensory modalities and exploratory procedures to improve haptic object recognition in robotics},
	doi = {10.1109/HUMANOIDS.2016.7803263},
	abstract = {In this paper we tackle the problem of object recognition using haptic feedback from a robot holding and manipulating different objects. One of the main challenges in this setting is to understand the role of different sensory modalities (namely proprioception, object weight from F/T sensors and touch) and how to combine them to correctly discriminate different objects. We investigated these aspects by considering multiple sensory channels and different exploratory strategies to gather meaningful information regarding the object's physical properties. We propose a novel strategy to train a learning machine able to efficiently combine sensory modalities by first learning individual object features and then combine them in a single classifier. To evaluate our approach and compare it with previous methods we collected a dataset for haptic object recognition, comprising 11 objects that were held in the hands of the iCub robot while performing different exploration strategies. Results show that our strategy consistently outperforms previous approaches [17].},
	booktitle = {2016 {IEEE}-{RAS} 16th {International} {Conference} on {Humanoid} {Robots} ({Humanoids})},
	author = {Higy, Bertrand and Ciliberto, Carlo and Rosasco, Lorenzo and Natale, Lorenzo},
	month = nov,
	year = {2016},
	keywords = {classifier, exploratory procedures, feedback, Force, force sensors, haptic feedback, haptic interfaces, Haptic interfaces, haptic object recognition, iCub robot hands, learning (artificial intelligence), machine learning, manipulators, multiple sensory channels, object features, object manipulation, object physical properties, object recognition, Object recognition, object weight, pattern classification, proprioception, Robot sensing systems, robotics, sensory modalities, Thumb, torque sensors},
	pages = {117--124},
	file = {Higy et al. - 2016 - Combining sensory modalities and exploratory proce.pdf:/home/bjrhigy/Zotero/storage/ZEJ44H7B/Higy et al. - 2016 - Combining sensory modalities and exploratory proce.pdf:application/pdf;higy_IROS_2016.pdf:/home/bjrhigy/Zotero/storage/YZINPW5J/higy_IROS_2016.pdf:application/pdf;humanoids_2016.zip:/home/bjrhigy/Zotero/storage/Z2Z8ZPCB/humanoids_2016.zip:application/zip},
}

@inproceedings{savran_energy_2018,
	address = {Xi'an, China},
	title = {Energy and {Computation} {Efficient} {Audio}-visual {Voice} {Activity} {Detection} {Driven} by {Event}-cameras},
	abstract = {We propose a novel method for computationally efficient audio-visual voice activity detection (VAD) where visual temporal information is provided by an energy efficient event-camera (EC). Unlike conventional cameras, ECs perform on-chip low-power pixel-level change detection, adapting the sampling frequency to the dynamics of the activity in the visual scene and removing redundancy, hence enabling energy and computational efficiency. In our VAD pipeline, first, lip activity is located and detected jointly by a probabilistic estimation after spatio-temporal filtering. Then, over the lips, a feather-weight speech-related lip motion detection is performed with minimum false negative rate to activate a highly accurate but expensive acoustic deep neural networks-based VAD. Our experiments show that ECs are accurate at detecting and locating lip activity; and EC-driven VAD can result in considerable savings in computations as well as can substantially reduce false positive rates in low acoustic signal-to-noise ratio conditions.},
	booktitle = {Proceedings of 13th {IEEE} {International} {Conference} on {Automatic} {Face} {Gesture} {Recognition}},
	author = {Savran, Arman and Tavarone, Raffaele and Higy, Bertrand and Badino, Leonardo and Bartolozzi, Chiara},
	month = may,
	year = {2018},
	keywords = {Cameras, Carnegie Mellon University, Charge coupled devices, Charge-coupled image sensors, CMU 3D Room, CMU PIE database, data acquisition, data collection procedure, database applications, face recognition, Face recognition, facial image database, Hardware, Head, Image databases, image processing equipment, Image storage, imaging hardware, lighting, Lighting, Pose, Illumination and Expression database, Robots, visual databases},
	file = {Savran et al. - 2018 - Energy and Computation Efficient Audio-visual Voic.pdf:/home/bjrhigy/Zotero/storage/YCH6IH9R/Savran et al. - 2018 - Energy and Computation Efficient Audio-visual Voic.pdf:application/pdf;savran_ecvad_ispc17.pdf:/home/bjrhigy/Zotero/storage/C2ZG92K5/savran_ecvad_ispc17.pdf:application/pdf;savran_sbm_v6.pdf:/home/bjrhigy/Zotero/storage/NXMT22Y4/savran_sbm_v6.pdf:application/pdf},
}

@inproceedings{chrupala_analyzing_2020,
	address = {Seattle, WA, USA},
	title = {Analyzing analytical methods: {The} case of phonology in neural models of spoken language},
	doi = {10.18653/v1/2020.acl-main.381},
	abstract = {Given the fast development of analysis techniques for NLP and speech            
processing systems, few systematic studies have been conducted to               
compare the strengths and weaknesses of each method.  As a step in              
this direction we study the case of representations of phonology in             
neural network models of spoken language. We use two commonly applied           
analytical techniques, diagnostic classifiers and representational              
similarity analysis, to quantify to what extent neural activation               
patterns encode phonemes and phoneme sequences. We manipulate two               
factors that can affect the outcome of analysis. First, we investigate          
the role of learning by comparing neural activations extracted from             
trained versus randomly-initialized models. Second, we examine the              
temporal scope of the activations by probing both local activations             
corresponding to a few milliseconds of the speech signal, and global            
activations pooled over the whole utterance. We conclude that                   
reporting analysis results with randomly initialized models is                  
crucial, and that global-scope methods tend to yield more consistent            
and interpretable results and we recommend their use as a complement            
to local-scope diagnostic methods.},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Chrupa{\l }a, Grzegorz and Higy, Bertrand and Alishahi, Afra},
	month = jul,
	year = {2020},
	file = {Chrupa{\l }a et al. - 2020 - Analyzing analytical methods The case of phonolog.pdf:/home/bjrhigy/Zotero/storage/KKBKDG9B/Chrupa{\l }a et al. - 2020 - Analyzing analytical methods The case of phonolog.pdf:application/pdf},
}

@inproceedings{higy_textual_2020,
	address = {Online},
	title = {Textual {Supervision} for {Visually} {Grounded} {Spoken} {Language} {Understanding}},
	url = {https://www.aclweb.org/anthology/2020.findings-emnlp.244},
	doi = {10.18653/v1/2020.findings-emnlp.244},
	abstract = {Visually-grounded models of spoken language understanding extract semantic information directly from speech, without relying on transcriptions. This is useful for low-resource languages, where transcriptions can be expensive or impossible to obtain. Recent work showed that these models can be improved if transcriptions are available at training time. However, it is not clear how an end-to-end approach compares to a traditional pipeline-based approach when one has access to transcriptions. Comparing different strategies, we find that the pipeline approach works better when enough text is available. With low-resource languages in mind, we also show that translations can be effectively used in place of transcriptions but more data is needed to obtain similar results.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Higy, Bertrand and Elliott, Desmond and Chrupa{\l }a, Grzegorz},
	month = nov,
	year = {2020},
	pages = {2698--2709},
	file = {arXiv Fulltext PDF:/home/bjrhigy/Zotero/storage/VTFLDSBC/Higy et al. - 2020 - Textual Supervision for Visually Grounded Spoken L.pdf:application/pdf},
}

@phdthesis{higy_spoken_2019,
	title = {Spoken command recognition for robotics},
	url = {https://iris.unige.it//handle/11567/942171},
	language = {en},
	urldate = {2020-12-04},
	author = {Higy, Bertrand},
	month = apr,
	year = {2019},
	doi = {10.15167/higy-bertrand_phd2019-04-08},
	note = {Publisher: Universit{\`a} degli studi di Genova},
	file = {Higy - 2019 - Spoken command recognition for robotics.pdf:/home/bjrhigy/Zotero/storage/7PZ66SYD/Higy - 2019 - Spoken command recognition for robotics.pdf:application/pdf},
}

@article{alishahi_zr-2021vg_2021,
	title = {{ZR}-{2021VG}: {Zero}-{Resource} {Speech} {Challenge}, {Visually}-{Grounded} {Language} {Modelling} track, 2021 edition},
	shorttitle = {{ZR}-{2021VG}},
	url = {http://arxiv.org/abs/2107.06546},
	abstract = {We present the visually-grounded language modelling track that was introduced in the Zero-Resource Speech challenge, 2021 edition, 2nd round. We motivate the new track and discuss participation rules in detail. We also present the two baseline systems that were developed for this track.},
	urldate = {2021-07-27},
	journal = {arXiv:2107.06546 [cs, eess]},
	author = {Alishahi, Afra and Chrupa{\l }a, Grzegorz and Cristia, Alejandrina and Dupoux, Emmanuel and Higy, Bertrand and Lavechin, Marvin and R{\"a}s{\"a}nen, Okko and Yu, Chen},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.06546},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/home/bjrhigy/Zotero/storage/Z6BH58TJ/Alishahi et al. - 2021 - ZR-2021VG Zero-Resource Speech Challenge, Visuall.pdf:application/pdf;arXiv.org Snapshot:/home/bjrhigy/Zotero/storage/ZM7JUTBZ/2107.html:text/html},
}

@article{higy_discrete_2021,
	title = {Discrete representations in neural models of spoken language},
	url = {http://arxiv.org/abs/2105.05582},
	abstract = {The distributed and continuous representations used by neural networks are at odds with representations employed in linguistics, which are typically symbolic. Vector quantization has been proposed as a way to induce discrete neural representations that are closer in nature to their linguistic counterparts. However, it is not clear which metrics are the best-suited to analyze such discrete representations. We compare the merits of four commonly used metrics in the context of weakly supervised models of spoken language. We compare the results they show when applied to two different models, while systematically studying the effect of the placement and size of the discretization layer. We find that different evaluation regimes can give inconsistent results. While we can attribute them to the properties of the different metrics in most cases, one point of concern remains: the use of minimal pairs of phoneme triples as stimuli disadvantages larger discrete unit inventories, unlike metrics applied to complete utterances. Furthermore, while in general vector quantization induces representations that correlate with units posited in linguistics, the strength of this correlation is only moderate.},
	urldate = {2021-09-20},
	journal = {arXiv:2105.05582 [cs, eess]},
	author = {Higy, Bertrand and Gelderloos, Lieke and Alishahi, Afra and Chrupa{\l }a, Grzegorz},
	month = sep,
	year = {2021},
	note = {arXiv: 2105.05582},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/home/bjrhigy/Zotero/storage/LC55SYND/Higy et al. - 2021 - Discrete representations in neural models of spoke.pdf:application/pdf;arXiv.org Snapshot:/home/bjrhigy/Zotero/storage/LGTXNM6U/2105.html:text/html},
}
