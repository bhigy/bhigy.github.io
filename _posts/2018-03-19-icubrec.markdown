---
title: iCubRec
subtitle: Command Recognition for the iCub plateform
layout: default
modal-id: 1
date: 2018-03-19
img: icubrec.png
thumbnail: icubrec-thumbnail.png
alt: Interactive Object Learning (IOL) demo using our command recognition system
project-date: November 2015 - October 2018
client: Italian Institute of Technology
category: Automatic speech recognition, Command spotting, Deep Learning, Robotics
description: PhD position under the supervision of <a href="https://www.iit.it/people/giorgio-metta">Giorgio Metta</a> and <a href="https://www.iit.it/people/leonardo-badino">Leonardo Badino</a>. This research has two main goals&#58; (1) to implement an efficient command recognizer for iCub and (2) to explore new ideas to improve speech recognition in the context of human-robot interactions. <br><br> Automatic speech recognition (ASR) technology has now reached almost human-level performance in some real-usage scenarios, such as close-microphone dictation of isolated sentences. However, recognizing commands for a humanoid robot is different from this usual context of application, and the system we are building should be able to adapt to this new domain. <br><br> As a first step toward this goal, I collected the <a href="https://robotology.github.io/natural-speech/vocub">VoCub dataset</a>, a dataset of commands addressed to iCub, which match our target scenario and will help us in training and testing the system. A baseline system has also been built and tested on the robot. I am now working on new ideas for domain adaptation, possibly including distillation, multitask learning or curriculum learing. <ul class="list-inline social-buttons">Ressources:<br><br><li><a href="https://github.com/robotology/natural-speech/tree/master/icubrec"><i class="fab fa-github"></i></a></li><li><a href="files/higy_frontiers_2018.pdf"><i class="fa fa-file-pdf"></i></a></li></ul>

---
